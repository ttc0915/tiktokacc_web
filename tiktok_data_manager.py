#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import asyncio
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class RegionData:
    """åœ°åŒºæ•°æ®ç»“æ„"""
    name: str
    countries: Dict[str, Dict[str, int]]  # country_code -> {bracket: count}
    total_accounts: int
    total_countries: int

@dataclass
class CountryData:
    """å›½å®¶æ•°æ®ç»“æ„"""
    code: str
    name_zh: str
    region: str
    brackets: Dict[str, int]  # bracket -> count
    total_accounts: int
    centroid: Tuple[float, float] = (0.0, 0.0)

@dataclass
class AggregatedData:
    """èšåˆæ•°æ®ç»“æ„"""
    generated_at: str
    data_source: str
    brackets: List[str]
    regions: List[str]
    countries: List[CountryData]
    totals: Dict[str, int]
    metadata: Dict[str, any]

class TikTokDataManager:
    """TikTokæ•°æ®ç®¡ç†å™¨"""
    
    # æ ‡å‡†ç²‰ä¸åŒºé—´
    STANDARD_BRACKETS = [
        '0-500', '500-1000', '1000-2000', '2000-3000', '3000-4000',
        '4000-5000', '5000-6000', '6000-7000', '7000-8000', '8000-9000', '10000+'
    ]
    
    # å›½å®¶ä»£ç åˆ°ä¸­æ–‡åæ˜ å°„
    COUNTRY_NAMES = {
        'US': 'ç¾å›½', 'CN': 'ä¸­å›½', 'JP': 'æ—¥æœ¬', 'GB': 'è‹±å›½', 'FR': 'æ³•å›½',
        'DE': 'å¾·å›½', 'KR': 'éŸ©å›½', 'IN': 'å°åº¦', 'BR': 'å·´è¥¿', 'RU': 'ä¿„ç½—æ–¯',
        'CA': 'åŠ æ‹¿å¤§', 'AU': 'æ¾³å¤§åˆ©äºš', 'IT': 'æ„å¤§åˆ©', 'ES': 'è¥¿ç­ç‰™',
        'MX': 'å¢¨è¥¿å“¥', 'TR': 'åœŸè€³å…¶', 'NL': 'è·å…°', 'TH': 'æ³°å›½',
        'SG': 'æ–°åŠ å¡', 'MY': 'é©¬æ¥è¥¿äºš', 'PH': 'è²å¾‹å®¾', 'VN': 'è¶Šå—',
        'ID': 'å°åº¦å°¼è¥¿äºš', 'PK': 'å·´åŸºæ–¯å¦', 'BD': 'å­ŸåŠ æ‹‰å›½', 'IQ': 'ä¼Šæ‹‰å…‹',
        'IR': 'ä¼Šæœ—', 'SA': 'æ²™ç‰¹é˜¿æ‹‰ä¼¯', 'AE': 'é˜¿è”é…‹', 'EG': 'åŸƒåŠ',
        'MA': 'æ‘©æ´›å“¥', 'DZ': 'é˜¿å°”åŠåˆ©äºš', 'TN': 'çªå°¼æ–¯', 'LY': 'åˆ©æ¯”äºš',
        'SD': 'è‹ä¸¹', 'ET': 'åŸƒå¡ä¿„æ¯”äºš', 'KE': 'è‚¯å°¼äºš', 'UG': 'ä¹Œå¹²è¾¾',
        'TZ': 'å¦æ¡‘å°¼äºš', 'ZA': 'å—é', 'NG': 'å°¼æ—¥åˆ©äºš', 'GH': 'åŠ çº³',
        'CI': 'ç§‘ç‰¹è¿ªç“¦', 'SN': 'å¡å†…åŠ å°”', 'ML': 'é©¬é‡Œ', 'BF': 'å¸ƒåŸºçº³æ³•ç´¢',
        'NE': 'å°¼æ—¥å°”', 'TD': 'ä¹å¾—', 'CM': 'å–€éº¦éš†', 'CF': 'ä¸­éå…±å’Œå›½',
        'CG': 'åˆšæœå…±å’Œå›½', 'CD': 'åˆšæœæ°‘ä¸»å…±å’Œå›½', 'AO': 'å®‰å“¥æ‹‰', 'ZM': 'èµæ¯”äºš',
        'ZW': 'æ´¥å·´å¸ƒéŸ¦', 'BW': 'åšèŒ¨ç“¦çº³', 'NA': 'çº³ç±³æ¯”äºš', 'MZ': 'è«æ¡‘æ¯”å…‹',
        'MG': 'é©¬è¾¾åŠ æ–¯åŠ ', 'MU': 'æ¯›é‡Œæ±‚æ–¯', 'SC': 'å¡èˆŒå°”', 'KM': 'ç§‘æ‘©ç½—',
        'DJ': 'å‰å¸ƒæ', 'SO': 'ç´¢é©¬é‡Œ', 'ER': 'å„ç«‹ç‰¹é‡Œäºš', 'SS': 'å—è‹ä¸¹',
        'unknown': 'æœªçŸ¥åœ°åŒº'
    }
    
    def __init__(self, data_path: str = None):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        self.data_path = Path(data_path) if data_path else None
        self.cache_file = Path("data_cache.json")
        self.config_file = Path("config.json")
        self._cache = {}
        self._last_scan_time = None
        
    def set_data_path(self, path: str) -> bool:
        """è®¾ç½®æ•°æ®è·¯å¾„"""
        try:
            self.data_path = Path(path)
            if not self.data_path.exists():
                logger.error(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {path}")
                return False
            
            # ä¿å­˜é…ç½®
            config = {
                'data_path': str(self.data_path.absolute()),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®è·¯å¾„è®¾ç½®æˆåŠŸ: {path}")
            return True
        except Exception as e:
            logger.error(f"è®¾ç½®æ•°æ®è·¯å¾„å¤±è´¥: {e}")
            return False
    
    def load_config(self) -> Optional[Dict]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if self.config_file.exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                if 'data_path' in config:
                    self.data_path = Path(config['data_path'])
                return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
        return None
    
    def scan_data_directory(self) -> Tuple[int, int, int]:
        """æ‰«ææ•°æ®ç›®å½•ï¼Œè¿”å›(åœ°åŒºæ•°, å›½å®¶æ•°, æ–‡ä»¶æ•°)"""
        if not self.data_path or not self.data_path.exists():
            return 0, 0, 0
        
        regions = 0
        countries = 0
        files = 0
        
        try:
            for region_dir in self.data_path.iterdir():
                if region_dir.is_dir():
                    regions += 1
                    for country_dir in region_dir.iterdir():
                        if country_dir.is_dir():
                            countries += 1
                            for file_path in country_dir.iterdir():
                                if file_path.is_file() and file_path.suffix == '.txt':
                                    files += 1
        except Exception as e:
            logger.error(f"æ‰«æç›®å½•å¤±è´¥: {e}")
        
        return regions, countries, files
    
    def read_account_file(self, file_path: Path) -> int:
        """è¯»å–è´¦å·æ–‡ä»¶ï¼Œè¿”å›è´¦å·æ•°é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            # è¿‡æ»¤ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            valid_lines = [line.strip() for line in lines 
                          if line.strip() and not line.strip().startswith('#')]
            return len(valid_lines)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return 0
    
    def parse_data_directory(self) -> AggregatedData:
        """è§£ææ•°æ®ç›®å½•ï¼Œç”Ÿæˆèšåˆæ•°æ®"""
        if not self.data_path or not self.data_path.exists():
            raise ValueError("æ•°æ®è·¯å¾„æœªè®¾ç½®æˆ–ä¸å­˜åœ¨")
        
        logger.info(f"å¼€å§‹è§£ææ•°æ®ç›®å½•: {self.data_path}")
        
        regions_data = {}
        countries_list = []
        total_accounts = 0
        
        for region_dir in self.data_path.iterdir():
            if not region_dir.is_dir():
                continue
            
            region_name = region_dir.name
            logger.info(f"å¤„ç†åœ°åŒº: {region_name}")
            
            region_countries = {}
            region_total = 0
            
            for country_dir in region_dir.iterdir():
                if not country_dir.is_dir():
                    continue
                
                country_code = country_dir.name
                country_brackets = {}
                country_total = 0
                
                # è¯»å–å„ä¸ªç²‰ä¸åŒºé—´æ–‡ä»¶
                for bracket in self.STANDARD_BRACKETS:
                    file_path = country_dir / f"{bracket}.txt"
                    if file_path.exists():
                        count = self.read_account_file(file_path)
                        if count > 0:
                            country_brackets[bracket] = count
                            country_total += count
                
                if country_total > 0:
                    # åˆ›å»ºå›½å®¶æ•°æ®
                    country_data = CountryData(
                        code=country_code,
                        name_zh=self.COUNTRY_NAMES.get(country_code, country_code),
                        region=region_name,
                        brackets=country_brackets,
                        total_accounts=country_total
                    )
                    
                    countries_list.append(country_data)
                    region_countries[country_code] = country_brackets
                    region_total += country_total
                    total_accounts += country_total
            
            if region_countries:
                regions_data[region_name] = RegionData(
                    name=region_name,
                    countries=region_countries,
                    total_accounts=region_total,
                    total_countries=len(region_countries)
                )
        
        # æŒ‰è´¦å·æ•°æ’åº
        countries_list.sort(key=lambda x: x.total_accounts, reverse=True)
        
        # ç”Ÿæˆèšåˆæ•°æ®
        aggregated = AggregatedData(
            generated_at=datetime.now().isoformat(),
            data_source=str(self.data_path.absolute()),
            brackets=self.STANDARD_BRACKETS,
            regions=list(regions_data.keys()),
            countries=countries_list,
            totals={
                'accounts': total_accounts,
                'countries': len(countries_list),
                'regions': len(regions_data)
            },
            metadata={
                'scan_time': datetime.now().isoformat(),
                'data_hash': self._calculate_data_hash(countries_list)
            }
        )
        
        logger.info(f"æ•°æ®è§£æå®Œæˆ: {len(countries_list)} ä¸ªå›½å®¶, {total_accounts} ä¸ªè´¦å·")
        return aggregated
    
    def _calculate_data_hash(self, countries: List[CountryData]) -> str:
        """è®¡ç®—æ•°æ®å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜åŒ–"""
        data_str = json.dumps([asdict(c) for c in countries], sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def save_cache(self, data: AggregatedData):
        """ä¿å­˜ç¼“å­˜"""
        try:
            cache_data = {
                'data': asdict(data),
                'cached_at': datetime.now().isoformat()
            }
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info("ç¼“å­˜ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def load_cache(self) -> Optional[AggregatedData]:
        """åŠ è½½ç¼“å­˜"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                data_dict = cache_data['data']
                # é‡æ„CountryDataå¯¹è±¡
                countries = [CountryData(**c) for c in data_dict['countries']]
                data_dict['countries'] = countries
                
                return AggregatedData(**data_dict)
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return None
    
    def get_data(self, use_cache: bool = True, force_refresh: bool = False) -> AggregatedData:
        """è·å–æ•°æ®"""
        if use_cache and not force_refresh:
            cached_data = self.load_cache()
            if cached_data:
                logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®")
                return cached_data
        
        # é‡æ–°è§£ææ•°æ®
        data = self.parse_data_directory()
        self.save_cache(data)
        return data
    
    def export_to_json(self, output_path: str = "aggregated.json") -> bool:
        """å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            data = self.get_data()
            output_data = {
                'generatedAt': data.generated_at,
                'brackets': data.brackets,
                'totals': data.totals,
                'countries': [
                    {
                        'code': c.code,
                        'nameZh': c.name_zh,
                        'region': c.region,
                        'centroid': list(c.centroid),
                        'byBracket': c.brackets,
                        'totals': {'accounts': c.total_accounts}
                    }
                    for c in data.countries
                ],
                'regions': data.regions
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
            return True
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ•°æ®å¤±è´¥: {e}")
            return False

class TikTokDataServer:
    """TikTokæ•°æ®æœåŠ¡å™¨"""
    
    def __init__(self, data_dir: str, output_file: str = None, port: int = 8080):
        """åˆå§‹åŒ–æœåŠ¡å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
            port: æœåŠ¡å™¨ç«¯å£
        """
        self.data_manager = TikTokDataManager(data_dir)
        self.output_file = output_file or "å¯è§†åŒ–tiktokè´¦å·åœ°çƒæ•°æ®/public/data/aggregated.json"
        self.port = port
        self.host = 'localhost'
        self.is_running = False
        
        # ç¡®ä¿æ•°æ®è·¯å¾„è®¾ç½®æ­£ç¡®
        if not self.data_manager.set_data_path(data_dir):
            raise ValueError(f"æ— æ³•è®¾ç½®æ•°æ®è·¯å¾„: {data_dir}")
    
    def run_forever(self):
        """å¯åŠ¨æœåŠ¡å™¨å¹¶æŒç»­è¿è¡Œ"""
        try:
            logger.info("ğŸš€ å¯åŠ¨TikTokæ•°æ®æœåŠ¡å™¨")
            logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_manager.data_path}")
            logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {self.output_file}")
            logger.info(f"ğŸŒ æœåŠ¡ç«¯å£: {self.port}")
            
            self.is_running = True
            
            # é¦–æ¬¡æ•°æ®è§£æå’Œå¯¼å‡º
            self.refresh_data_file()
            
            logger.info("âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
            logger.info("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
            
            # æŒç»­è¿è¡Œï¼Œæ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ•°æ®æ›´æ–°
            try:
                while self.is_running:
                    import time
                    time.sleep(30)
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªåŠ¨åˆ·æ–°é€»è¾‘
                    logger.info("ğŸ”„ å®šæœŸæ£€æŸ¥æ•°æ®æ›´æ–°...")
                    
            except KeyboardInterrupt:
                logger.info("\nâ¹ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
                self.stop()
                
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡å™¨è¿è¡Œå¤±è´¥: {e}")
            raise
    
    def refresh_data_file(self) -> bool:
        """åˆ·æ–°æ•°æ®æ–‡ä»¶"""
        try:
            logger.info("ğŸ”„ å¼€å§‹åˆ·æ–°æ•°æ®...")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
            
            # å¯¼å‡ºæ•°æ®åˆ°æ–‡ä»¶
            success = self.data_manager.export_to_json(self.output_file)
            
            if success:
                logger.info(f"âœ… æ•°æ®å·²æˆåŠŸå¯¼å‡ºåˆ°: {self.output_file}")
                return True
            else:
                logger.error("âŒ æ•°æ®å¯¼å‡ºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆ·æ–°æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def start(self, host: str = 'localhost', port: int = None):
        """å¯åŠ¨æœåŠ¡å™¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        if port:
            self.port = port
        if host:
            self.host = host
            
        logger.info(f"TikTokæ•°æ®æœåŠ¡å™¨å‡†å¤‡å¯åŠ¨åœ¨ {self.host}:{self.port}")
        self.is_running = True
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„HTTPæœåŠ¡å™¨å®ç°
        # ä¾‹å¦‚ä½¿ç”¨aiohttpæˆ–fastapi
        logger.info("æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        try:
            while self.is_running:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("æœåŠ¡å™¨åœæ­¢")
            self.is_running = False
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        self.is_running = False
        logger.info("ğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")
    
    def get_status(self) -> Dict:
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        return {
            'status': 'running' if self.is_running else 'stopped',
            'data_path': str(self.data_manager.data_path) if self.data_manager.data_path else None,
            'output_file': self.output_file,
            'port': self.port,
            'timestamp': datetime.now().isoformat()
        }
    
    def refresh_data(self) -> Dict:
        """åˆ·æ–°æ•°æ®"""
        try:
            data = self.data_manager.get_data(force_refresh=True)
            
            # åŒæ—¶æ›´æ–°è¾“å‡ºæ–‡ä»¶
            self.refresh_data_file()
            
            return {
                'success': True,
                'message': 'æ•°æ®åˆ·æ–°æˆåŠŸ',
                'data': asdict(data),
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'success': False,
                'message': f'æ•°æ®åˆ·æ–°å¤±è´¥: {str(e)}',
                'timestamp': datetime.now().isoformat()
            }

# ä¸»ç¨‹åºå…¥å£
def main():
    """ä¸»ç¨‹åº"""
    print("ğŸš€ TikTokæ•°æ®ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = TikTokDataManager()
    
    # å°è¯•åŠ è½½é…ç½®
    config = manager.load_config()
    if config and 'data_path' in config:
        print(f"ğŸ“ å·²åŠ è½½é…ç½®ï¼Œæ•°æ®è·¯å¾„: {config['data_path']}")
        regions, countries, files = manager.scan_data_directory()
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {regions} ä¸ªåœ°åŒº, {countries} ä¸ªå›½å®¶, {files} ä¸ªæ–‡ä»¶")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¯·è®¾ç½®æ•°æ®è·¯å¾„")
        data_path = input("è¯·è¾“å…¥æ•°æ®ç›®å½•è·¯å¾„: ").strip()
        if data_path and manager.set_data_path(data_path):
            print("âœ… æ•°æ®è·¯å¾„è®¾ç½®æˆåŠŸ")
        else:
            print("âŒ æ•°æ®è·¯å¾„è®¾ç½®å¤±è´¥")
            return
    
    # å¯¼å‡ºæ•°æ®
    output_path = "å¯è§†åŒ–tiktokè´¦å·åœ°çƒæ•°æ®/public/data/aggregated.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    if manager.export_to_json(output_path):
        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
    else:
        print("âŒ æ•°æ®å¯¼å‡ºå¤±è´¥")

if __name__ == "__main__":
    main() 